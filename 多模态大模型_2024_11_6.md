[TOC]



# Woodpecker: Hallucination Correction for Multimodal Large Language Models

> 现在幻觉的减少方法主要是instruction-turnning manner ，因此引入免训练方式

由五个部分组成：

- key concept extraction：识别生成句子中提到的主要对象

- question formulation：围绕提取出的对象提出问题，如其数量和属性
- visual knowledge validation：通过专家模型回答提出的问题。例如，可使用视觉感知模型来确定对象数量
- visual claim generation：将上述问题-答案（QA）对转换为视觉知识库，该知识库由关于输入图像的对象级和属性级claims组成
-  hallucination correction：根据视觉知识库，修改幻觉并添加相应的证据

>  woodpecker可以轻松地为不同的 MLLM 服务，同时可以通过访问五个阶段的中间输出进行解释
>
> dataset：POPE、MME、LLaVA-QA90

**主要贡献**

- 第一个采用矫正方式来解决视觉幻觉问题的
- 我们的框架设计方式使每一步都清晰透明，从而提供了良好的可解释性
- 对我们的方法的有效性进行了全面评估，结果表明，该方法在幻觉矫正方面的巨大潜力（on the POPE bench-mark, our method largely boosts the accuracy of the baseline MiniGPT-4 [47]/mPLUG-Owl [40] from 54.67%/62% to 85.33%/86.33%.）

## Related Work

## Method

![](D:\Code\AI\asset\Snipaste_2024-11-17_21-59-25.png)

### Key Concept Extraction

- 例如，给定一个句子 “这个男人戴着一顶黑色的帽子”，“男人 ”和 “帽子 ”这两个对象就会被提取出来，并在接下来的步骤中作为诊断的中心

### Question Formulation



### Visual Knowledge Validation

- object-level：employ an open-set object detector as the solver
- attribute-level：apply a pre-trained VQA model to answer the questions conditioned on the image（与主流的 MLLM 相比，VQA 模型生成的答案往往更短，但也会产生较少的幻觉，因此是一种合理的选择。）

### Visual Claim Generation

在提出和回答问题后，我们将 QA对组合成可视化主张，并将其整理成可视化知识库，供下一步参考

- Object-level claims：

  - For existing objects, we add a claim as “There are {counts} {name}.”, where “{counts}” and “{name}” are the counts and the name of a certain kind of object
  - “There is no {name}”, for nonexistent objects

- Attribute-level claims：

  - 采用 QA 到claims模型 [10] 将问题和答案合并为claims

  - 为了处理涉及多个物体或前景物体与背景之间关系的情况，需要更多的全局信息。因此，我们也将涉及不同对象之间或对象与背景之间交互的主张纳入其中



### Hallucination Correction

combining the visual knowledge base with the original responses into a prompt, we instruct an LLM to correct the responses and output the refined ones（为了提高可解释性，我们明确指示 LLM 在引用表达式时，在表达式的右后方附加边界框）



## Experiment

### Experimental Settings

- Dataset. 

  - POPE：
    - 它包含随机抽样、流行抽样和对抗抽样的设置、和对抗采样，它们的主要区别在于构建负样本的方式不同
    - 随机采样图像中未出现的对象会被随机抽样
    - 在流行设置中，不存在的对象是从频率最高的对象库中抽取的
    - 在对抗设置中，图像中出现频率最高但在图像中并不存在的物体进行采样
    - 在采样设置方面，我们对 50 幅图像进行采样，并为每幅图像设置 6 个问题
    - 正负样本对比例 1：1
    - 重点评估对象层面的幻觉，更具体地说，是存在方面的幻觉

  - MME：
    - 包括十个感知能力子任务和四个认知能力子任务
    - 在本文中，我们对数据集进行了重新利用，选择了存在和计数子集来测量对象层面的幻觉。位置和颜色子集用于测量属性级幻觉。与 POPE 的设置类似，每个子集都由 “是或否 ”问题组成

  - LLaVA-QA90：
    - 我们对 10 个描述型查询进行了抽样，这些查询以不同的形式进行解析，以指示 MLLM 描述图像，如 “描述下面的图像 ”和 "照片是关于什么的？
    - LLaVA-QA90 使用 COCO  中的图像，并采用纯文本 GPT-4  来编写查询和参考答案
    - 我们舍弃参考答案，直接将图像输入 GPT-4V [26]，并提示它根据我们设计的两个维度对回答进行评分（准确性和详细性）

- Baselines.：

- Implementation Details.：

  - 选择 LLM、GPT-3.5-turbo、来完成关键概念提取、问题表述和幻觉纠正等子任务
  - 使用 Grounding DINO [22] 以默认检测阈值提取对象计数信息
  - 使用 BLIP-2-FlanT5XXL [16] 作为 VQA 模型，回答以输入图像为条件的属性相关问题
  - 对于 “是或否 ”的问题，我们发现一些 MLLM 的指令遵循能力较弱，经常输出无关的文本。另一方面，有些 MLLM 只输出单一的 “是 ”或 “否”，这也给校正带来了挑战
    - 我们首先从作为答案的再回答中提取关键词，即 “是 ”和 “否”，然后将问题和答案合并为更具体的主张。例如，给定一个问题 "图片中有狗吗？我们将更具体的答案组合为“是的，图像中有一只狗。“
    - 在修正过程中，我们额外将问题反馈给 LLM，以便 LLM 能够更好地掌握上下文和任务要求

### Experimental Results

- POPE
  - 在随机设置下，MiniGPT-4 的感知能力相对较弱，特别是在判断物体是否存在
  - mPLUG-Owl 和 Otter 往往过于自信，高 “是 ”率就反映了这一点。同时，高召回率和低精确度导致 f1 分数相对较低
  - 在 MiniGPT-4 和 mPL-4 中分别获得了 30.66% 和 24.33% 的相对增益
  - 在更具挑战性的流行和对抗设置中，MLLMs 在不同程度上表现出性能下降，在相对较强的基线（如 LLaVA）中更为突出。这种趋势表明，MLLM 可能会错误地适应训练语料中的某些数据特征（例如，流行设置中的下降可能源于长尾数据分布 。与此相反，在配备了稳健的专家视觉模型，我们的校正方法显示出很强的稳定性。各种指标都有明显改善，所有准确率都超过了 80%）

![](D:\Code\AI\asset\Snipaste_2024-11-17_22-40-53.png)

- MME
  - 在属性级评估方面，基线 MLLM 往往会取得较差的结果，这表明它们更容易产生属性级幻觉
  - 位置方面的改进相对较小，这可能是由以下两个因素造成的：（1）VQA 模型 BLIP-2 在位置推理方面的能力相对较弱（2）LLM可能无法很好地理解给定的边界框，从而无法自行推导出位置关系



![](D:\Code\AI\asset\Snipaste_2024-11-17_23-05-11.png)

- LLaVA-QA90

![](D:\Code\AI\asset\Snipaste_2024-11-18_09-10-32.png)

### Experimental Analysis

**Analysis of framework modules**

![](D:\Code\AI\asset\Snipaste_2024-11-18_09-12-00.png)

- default w/Detector、 default w/VQA前者仅通过在知识库中提供对象级信息来实现，而后者则通过提供属性级信息来实现

**Analysis of correction performance**

- Accuracy: | correct answers kept and wrong answers corrected |  | problems |.
- Omission: | wrong responses that fail to be corrected | | problems |.
- Mis-correction: | correct responses mistakenly modified | | problems |.

![](D:\Code\AI\asset\Snipaste_2024-11-18_09-17-45.png)

# Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning

我们在设计 LRV-Instruction 时同时考虑了正面和负面指令，以实现更稳健的视觉指令调整

- 引入GPT4-Assisted Visual Instruction Evaluation (GAVIE)来评估模型（相关性用于评估指令跟随性能，准确性用于测量 LMM 输出中的视觉幻觉）

![](D:\Code\AI\asset\Snipaste_2024-11-18_10-30-36.png)

> 蓝色表示LMM无法遵从人类指令、红色表示它们存在幻觉问题
>
> 我们的实验表明，GAVIE 不仅稳定，而且与人类的评估结果一致

我们的负面指令在三个语义层次上生成： (i) 不存在的物体操作；(ii) 存在的物体操作；(iii) 知识操作

- 在我们的评估集和公共基准 上，我们的经过指令调整的 LMM 受幻觉影响更小，性能达到了一流水平

## LRV-Instruction

一个栗子：

![](D:\Code\AI\asset\Snipaste_2024-11-18_11-22-50.png)

LRV-Instruction 的综合统计。在（d）中，蓝色表示操作存在的物体，粉色表示不存在的对象操作，绿色表示知识操作。

![](D:\Code\AI\asset\Snipaste_2024-11-18_11-37-27.png)

## Experiment













