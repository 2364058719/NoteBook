#### Transformer

> 最早用于机器翻译的NLP领域
>
> transformer的block可以保证input和output具有相同维度
>
> 可以自己学习长距离的semantic level的关联

#### BERT

> 为NLP的预训练而设计，使用Transformer，双向结构，不同于GPT模型的单向（但是GPT是生成模型，应用不同）
>
> 从无标签的文本中获取双向表征
>
> 能被fine-tune，end2end
>
> - 论文中提到标准模型单向限制了微调
> - BERT适合句子级以及令牌级的任务
> - MLM（完形填空）的自监督loss，对后续的许多多模态的模型都有应用，适合pre-train
> - 引入CLS用于获取全局特征

#### ViT

> Vision Transformer
>
> - Transformer缺少归纳偏置 locality、变换等价
> - 论文中融合了19个数据集来证明其鲁棒性
> - 将图片打成一个个16*16的patchs，通过一层linear projection获得image embedding。并学习BERT中使用posision embedding和CLS获取全局特征的方式。通过n个transformer encoder，最后用一个多分类头做下游任务。
> - 提出一种Hybrid模型，前面使用CNN代替linear projection抽取image特征，然后再使用ViT，效果更好
> - 但是在fine-tune和推理时，可能出现图片尺寸不同的情况，可以通过2D插值
> - 在消融实验中，比较ResNet和不同参数大小的ViT时，横轴代表不同大小数据集与训练样本数，所有样本抽取自JFT，减少了distribution
> - challenge：1）距离当时的有监督模型有差距  2）缺少自监督的预训练方式

#### MAE

> mask随机一些patches，然后做自监督训练，效果很好。
>
> 图片中有很多冗余信息，所以遮住大部分patches仍然能训练的很好
>
> 编码器只看见patches，解码器重构masked patches
>
> 实现简单，并且在训练1600个epochs后准确度仍然在上升
>
> challenges：1）可能学习到负面的bias   2）可能生成不存在的内容

#### MoCo

> 最具贡献的地方，1）使用queue提供了一个更大的字典，有更多负样本对可以对比学习，将字典大小与mini-batch剥离。2）使用momentum encoder，尽量保持了queue中key经过encoder之后的一致性。
>
> 使用InfoNCE作为loss function（这个地方没太看懂，到时候读下代码）

#### CLIP

> - 对于下游任务的zero-shot能力强
> - 使用对比学习预训练，通过相似矩阵计算loss
> - 使用prompt engineering，保持推理时，文本的一致性
> - 摆脱了categolrical label的限制，可以预测未训练过的分类
> - 可以直接将CLIP预训练之后用于作为一个特征抽取的块
> - 泛化性好

#### Two-stream网络

> 视频领域的开山之作
>
> - 对于不好学习的特征，可以手工添加一个流抽取出来

#### I3D

> 将imagenet预训练的2D参数膨胀为3D模型，方便预训练
>
> 同样使用two-stream的形式，光流读取时间特征，另一个读取空间特征
>
> 视频理解时，避免下采样，因为本来读取帧数的时长短

#### DERT

> - nms使object detection方向很少有end2end的方式，而DERT通过二分图匹配，避免了nms
> - 通过CNN抽取image特征，然后使用transformer encoder-decoder生成box，使用二分图匹配去掉nms操作
>
> - DERT用于全景分割下游任务效果很好
> - 可学习的object query，这些是decoder的位置编码查询，最后学习到的是一些box的位置与形状信息

#### DALLE2

> - 使用扩散模型（扩散模型中使用了U-Net结构，使输出和输入保持一个维度，我认为transformer也有相同的效果可以替换）
> - text----(CLIP)-----> text embedding -----(Prior)--->image embedding----(decoder)---->image
> - 缺少细节特征的生成

#### ViLT

> - 不需要使用CNN和Region Supervision，极大的减少了推理是所需要的时间和计算，使用一层Visual embed代替
> - 对于CLIP中image和text融合时所用的点乘，换成一个可学习的transformer，用于融合多模态
> - 使用image的数据增强，除了cutout和color inversion，保证了image和text的匹配

#### ALBEF

> 通过momentum distillation提高了利用noisy的web data的使用
>
> 使用ICT、ITM、MLM三个loss，其中ICT和MLM都使用了momentum中pseudo-targets作为loss的一部分
>
> 在Vision--Language下游任务中取得不错成绩，image-text检索效果好



#### 总结

​	这一周主要粗略看了些相关的久的经典模型，但是当时并没有对于每篇论文及时总结，这个总结是后面补的，以后会对每篇论文提取一些trick或者启发点，并总结其challenge作为研究方向，并且要尽量复现代码。

​	对于每个模型的代码实现的还不清楚，虽然每篇论文都有很好理解的模型架构图，但是对于如何从代码实现是下周的主要任务

​	下周将先学习pyTorch，然后这些开源了代码的模型进行复现，便于理解消化