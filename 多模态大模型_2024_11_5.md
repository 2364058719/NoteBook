# 2024_11_5_weiditang

[TOC]

## Visual Instruction Tuning（LLaVA）

LLaVA

- an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general-purpose visual and language understanding

- 首次尝试将instruction-tuning扩展到语言-图像多模态空间

> 1、present a data reformation perspective and pipeline to convert image-text pairs into an appropriate instruction-following format, using ChatGPT/GPT-4
>
> 2、使用CLIP的Visual encoder和LLM连接，在Science QA数据集上实现SOTA
>
> 3、LLaVA-Bench：Multimodal instruction-following benchmark （LLaVA-Bench (COCO)、LLaVA-Bench (In-the-Wild)）

Instruction Tuning： visual instruction tuning is different from visual prompt tuning [23]: the former aims to improve the model’s instruction-following abilities, while the latter aims to improve the parameter-efficiency in model adaptation



### GPT-assisted Visual Instruction Data Generation

对于一幅image Xv和标题Xc，创建一组Xq

最简单的图像文本对data：  **Human : Xq Xv<STOP> Assistant : Xc<STOP>**

- 标题通常从不同角度描述视觉场景
- 边框通常定位场景中的物体，每个边框编码物体概念及其空间位置

![](D:\Code\AI\asset\1.png)

>  图片并没有展示给GPT



### Visual Instruction Tuning

#### Architecture

![](D:\Code\AI\asset\2.png)

	#### Training

![](D:\Code\AI\asset\3.png)

使用LLm原始的自回归目标 

![](D:\Code\AI\asset\4.png)

> only green sequence/tokens are used to compute the loss in the auto-regressive model

![](D:\Code\AI\asset\5.png)

**a two-stage instruction-tuning procedure**

- Stage 1: Pre-training for Feature Alignment

  冻结视觉编码器和 LLM 权重，只使用可训练参数 θ = W（投影矩阵）来最大化似然 (3)。图像特征 Hv 就能与预先训练的 LLM 词嵌入对齐。这一阶段可以理解为为冻结的 LLM 训练一个兼容的视觉标记器。

- Stage 2: Fine-tuning End-to-End

  - Multimodal Chatbot：conversation is multi-turn while the other two are single-turn
  - Science QA：模型provides the reasoning process in natural language and selects the answer among multiple choices。将数据组织为单轮对话，问题和上下文为 Xinstruct，推理和答案为 Xa。



> 按照 Vicuna 的超参数[9]，用 8×A100s 训练所有模型。我们以 2e-3 的学习率和 128 的batchsize在经过过滤的 CC-595K 子集上对模型进行了 1 个 epoch 的预训练，并在拟议的  LLaVA-Instruct-158K 数据集上进行了微调，学习率为 2e-5，batchsize大小为 32。

#### Limitation

LLaVA perceives the image as a “bag of patches”, failing to grasp the complex semantics within the image

- 下面这个例子中，LLaVA认为图片中有草莓味酸奶（然而只有草莓和酸奶）

![](D:\Code\AI\asset\6.png)

#### Ablation（on Science QA）

![](D:\Code\AI\asset\7.png)



## Video-LLaVA: Learning United Visual Representation by Alignment Before Projection（泛读）

- 能联合处理image和video
- 共同训练image和video，大大提高在1个epoch的收敛速度

![](D:\Code\AI\asset\9.png)

### Architecture

![](D:\Code\AI\asset\8.png)

### Limitation

- Video-LLaVA处理长视频能力弱（从每个视频中抽取8帧进如encoder）



## Multimodal Chain-of-Thought Reasoning in Language Models（MM-CoT）

- Multimodal-CoT that incorporates language (text) and vision (images) modalities into **a two-stage framework** that separates rationale generation and answer inference.
- Multimodal-CoT 具有减轻幻觉和提高收敛速度的优势

- MM-CoT将多步骤问题分解为中间推理步骤（理由），然后推导出推理结果

### Challenge of Multimodal-CoT

- 讨论了1B参数大小下的模型，CoT为什么会失效，并且解决方法

1、the model exceeds the maximum token limits before obtaining the required answer or stops generating the prediction early

2、Misleading by Hallucinated Rationales

![](D:\Code\AI\asset\10.png)

### Multimodal-CoT

**Architecture**

![](D:\Code\AI\asset\11.png)

> Rationale Generation 和 Answer Inference 共享模型结构，但输入 X 和输出 Y 有所不同。

- 分别训练两个模型，X → R、XR → A进行监督学习
- 推理时，在给定 X 的情况下，使用第一阶段训练的模型生成测试集的理由；这些理由将用于第二阶段的答案推理。

<img src="D:\Code\AI\asset\12.png" style="zoom: 67%;" />

#### Encoder

<img src="D:\Code\AI\asset\13.png" style="zoom: 67%;" />

- LanguageEncoder(·)使用Transformer实现
- VisionExtractor(·)使用Vision Transformer实现，fetch the patch-level features by frozen vision extraction models

- 对于没有图片的输入X，我们使用相同大小的“blank features”代替

#### Interaction

- 使用single-head attention network
- Q、K、V分别使用H_language、H_vision、H_vision

<img src="D:\Code\AI\asset\15.png" style="zoom:60%;" />

<img src="D:\Code\AI\asset\14.png" style="zoom:60%;" />

#### Decoding

-  the fused output H_fuse is fed into the Transformer decoder to predict the target Y

### Experiments

> Datasets：ScienceQA、A-OKVQA

**3类baseline**

- Visual question answering (VQA) models
- LMs
- Fine-tuned large vision-language model

![](D:\Code\AI\asset\16.png)

### Analysis

1、CoT有助于提高收敛速度和减少幻觉

2、CoT与model具有正交性

3、使用LLM生成rationale训练MM-CoT也能使模型work

![](D:\Code\AI\asset\17.png)

4、Generalization to Other Multimodal Reasoning Benchmarks

5、对齐是重要的

6、错误分析

**Enhancements can be made to Multimodal-CoT by**

- 整合更多信息丰富的视觉特征，加强语言与视觉之间的交互
- 加入常识性知识
- implementing a filtering mechanism, such as using only relevant CoTs to infer answers and disregarding irrelevant ones.



## DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models

> - Labor-intensive annotation：手动注释推理耗时、费力，而且难以确保一致性和完整性
> - Flexibility：现有方法生成的多模态理由的有效性仅限于zero-shot prompt或使用 LM 进行微调学习（the rationales generated by MM-CoT fail to provide any benefits for zero-shot prompting）。
> - Generalizability：对需要新颖和未知推理路径的问题的通用性有限。此外，在特定子集（包括自然科学、社会科学和语言科学中任意两个学科的问题）上训练的模型在应用于另一个学科的问题时表现较差（zero-shot能力差）
> - Explainability：目前生成的理由的可解释性仍有待进一步提高

**DDCoT**

- 在理由生成过程中明确指出不确定性至关重要，因为这有助于提高生成理由的正确性，而这些理由是指导 LM 在推断答案的思维过程中进行思考的关键输入，particularly in the zero-shot setting
- 促使 LLM 明确确定推理和识别责任，以克服从交错的多模态输入直接生成推理时产生的语言幻觉问题。提出了一个由负空间提示、视觉识别和联合推理的顺序来进行交错推理和识别

**贡献**

- This work is the first to study the zero-shot multimodal rationale generation。深入分析了多模态 CoT 在理由生成方面的挑战和启示：zero-shot 提示中的理由敏感性、灾难性遗忘导致的微调所需的知识，以及交错多模态输入导致的幻觉强化
- 出了一种新颖的 DDCoT 提示，通过负空间设计和解构的共同作用，保持批判态度并确定推理和识别责任。由此产生的理由可以直接作为多模态输入的一部分，在 zero-shot 和微调学习中提高 LM 的推理能力

### Method

#### Motivation: Leverage Rationales for Multimodal Reasoning 

- Two-step Reasoning Process: Multimodal Rationales Generation and Utilization

  - explore to provide the rationale as **a structured logical chain** to explicitly guide the understanding of the image

- Roles of Rationales Differ in Zero-shot and Fine-tuning Learning

  - zero-shot 倾向于根据输入的 rationales 进行推理

    ![](D:\Code\AI\asset\18.png)

  - Knowledge-required reasoning for fine-tuning（比zero-shot 容错能力更强，同时微调模型对知识缺陷的敏感性也有所提高。这源于 LM 微调过程中的灾难性遗忘）

- Hallucinations Intensified

  - Uni-modal rationales have limited effect
  - Interleaved information exacerbates hallucinations

#### **Zero-shot DDCoT Prompting for Multimodal Rationale Generation**

> - Utilize two-step reasoning process
> - Generate rationales that meet the requirements of both zero-shot and fine-tuning learning, filled with knowledge and requiring fidelity
> - Alleviate the intensified hallucinations from interleaved information

3 steps:

1、utilize LLMs’ intrinsic knowledge to generate multimodal rationales.

2、明确提示 LLMs 逐步区分推理和识别的责任

3、明确标出不确定部分的负空间，强调理由生成过程中的批判性思维

![](D:\Code\AI\asset\19.png)

> - **Breaking Reasoning Down to Recognition Steps with Negative-Space Prompting**. 
>   - 第一步：employ the instruction “please think step-by-step and deconstruct the question down to necessary sub-questions" to obtain the sub-question sequence at once
>   - 第二步：明确提示 LLMs 判断每个子问题是否可以在没有视觉信息的情况下回答。为模型提供了以下提示： “假设你没有关于图片的任何信息，试着回答该子问题，如果无法确定该子问题，则将相应的子答案表述为'不确定'"。（“uncertainty” as a negative space）
>   - 通过对子问题采取批判的立场，并引入明确的不可见性假设在处理涉及图像的子问题时，我们成功地减轻了 LLM 中的幻觉。减少了事实错误。
> - **Visual Recognition to Obtain Visual Complements**
>   - employ the visual question answering (VQA) model to individually answer the sub-questions with negative space
> - **Integrate to Joint Reasoning**.
>   - 将获得的子问题和相应的子答案作为补充信息，促使 LLM “逐步思考”，利用语言和视觉信息进行联合推理，并生成多模态理由
>   - prompt explicitly with “note that the supplementary information given may not always be valid” and “select
>     valid information to form the rationale” 以鼓励对补充信息采取批判态度

#### **Utilization of Multimodal Rationales in General Scenarios**

- For zero-shot learning, we consider prompting the model with rationales, including established knowledge and a reasoning chain of thought
- for fine-tuning, our proposed rationales, in combination with our proposed **deep-layer prompting** and **rationale-compressed visual embedding**, improve deep multimodal understanding and reasoning.
  - DLP：It employs not only learnable prompts to facilitate the alignment of visual and linguistic semantics at a shallow level  but also utilizes explicit rationales to jointly encode multimodality by learning different prompts for each encoder layer 
  - RCVE：leverage the rationales to jointly comprehend the textual and visual contexts, serving as the prior knowledge to filtering visual features.

### Experiment

![](D:\Code\AI\asset\20.png)

![](D:\Code\AI\asset\21.png)

> 参数数量有限的模型在小型数据集上对齐图像和文本时会遇到困难

![](D:\Code\AI\asset\22.png)

![](D:\Code\AI\asset\23.png)

![](D:\Code\AI\asset\24.png)

### Limitation

- 幻觉改善
- 使用额外的image-text pair 来预训练视觉和语言之间的对齐
- bias



## Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models（GoT）

>  establishing a new state-of-the-art on ScienceQA test set with far fewer parameters.

### Graph-of-Thought

![](D:\Code\AI\asset\25.png)

#### GoT Construction

- propose a novel Extract-Clustering-Coreference (ECC) process to construct thought graphs

<img src="D:\Code\AI\asset\26.png" style="zoom:67%;" />

#### GoT Encoding and Integration

- utilizes separate encoders to encode input data for each modality

<img src="D:\Code\AI\asset\27.png" style="zoom:67%;" />

#### Feature Fusion

![](D:\Code\AI\asset\28.png)

![](D:\Code\AI\asset\29.png)

### Experiments

> - initialized our model with the finetuned T5 checkpoint FLAN-Alpaca 3 and used ViT-large encoder
>
> - fine-tuned the models for 100 epochs with a learning rate of 5e-5

### Results and Discussion

![](D:\Code\AI\asset\30.png)

> In conclusion, our results confirm the effectiveness of utilizing two-dimensional graph-of-thought and demonstrate the potential of incorporating GoT into reasoning for LMs.

#### Ablation Study

![](D:\Code\AI\asset\31.png)

- Random Thought Graph：这种方法旨在评估 GoT 推理机制在多大程度上依赖于思维图的结构化组织
- Triplets Concatenation：这种方法旨在评估省略通常由思维图提供的结构信息所产生的影响，从而深入了解构元素在推理过程中的重要性
- Coreference Injection：本实验旨在了解核心参照解析在增强模型演绎能力方面的作用

> (1) 简单地重新替换核心参照实体可能会导致句子失去连贯性，从而减少语义信息，因此对整体准确性的影响有限。
>
> (2) 用于核心参照解析的开放式信息提取（OpenIE）并非完美无缺，直接替换实体可能会带来噪音，从而在判断过程中误导语言模型。

![](D:\Code\AI\asset\32.png)

#### Analysis

![](D:\Code\AI\asset\33.png)

![](D:\Code\AI\asset\34.png)

















